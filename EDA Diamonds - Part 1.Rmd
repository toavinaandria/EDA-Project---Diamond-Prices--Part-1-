---
title: "Diamonds - EDA Project - Diamond Prices (Part 1)"
author: "Toavina Andriamanerasoa"
output: 
  html_document:
    theme: spacelab
    highlight: tango
---

This analysis represents the first part of the EDA project looking at diamond prices. I will first start by loading the relevant libraries and the diamonds dataset into a tableframe named diamonds:

```{r message=FALSE}
# Load the relevant libraries and the diamonds dataset into a dataframe
library(ggplot2)
library(dplyr)
library(tidyr)
data(diamonds)
diamonds <- tbl_df(diamonds)
```

## Problem A - Diamonds

### a) How many observations are in the data set?
There are ```r nrow(diamonds)```observations.

### b) How many variables are in the data set?
There are ```r ncol(diamonds)``` variables in the data set.

### c) How many ordered factors are in the data set?
Three - cut, color and clarity, as the other variables have significant number of unique values. I used the following functions to find answers to the above.
```{r}
nrow(diamonds)
ncol(diamonds)
summarise_each(diamonds, funs(n_distinct))
```

### d) What letter represents the best color for a diamond?
D, as shown by looking at function ```{r echo=FALSE} ?diamonds```


## Problem B - Create a histogram of the price of all the diamonds in the diamond data set
```{r}
ggplot(aes(x=price), data = diamonds)+
  geom_histogram(color = 'black', fill = '#F79420', binwidth = 500)
```

### Describe the shape and center of the price distribution. Include summary statistics like the mean and the median

The distribution is skewed to the left and has a fat tail towards higher prices - as a result the mean of ```r mean(diamonds$price)``` is significantly higher than the median of ```r median(diamonds$price)``` 

## Problem C - Diamond counts

### a) How many diamonds cost less than $500?
```r sum(diamonds$price<500)``` diamonds

### b) How many diamonds cost less than $250?
```r sum(diamonds$price<250)```

### c) How many diamonds cost $15,000 or more?
```r sum(diamonds$price>=15000)```

NB: The code below was used to show the outputs above
```{r}
sum(diamonds$price<500)
sum(diamonds$price<250)
sum(diamonds$price>=15000)
```

## Problem D - Explore the largest peak in the price histogram created earlier
```{r warning=FALSE}
ggplot(aes(x=price), data = diamonds)+
  geom_histogram(color = 'black', fill = '#F79420', binwidth = 50)+
  scale_x_continuous(lim=c(250,2000), breaks = seq(200,2000,100))
```

It is apparent that the peak has a median of around $700.   
  
## Problem E - Break out the histogram of diamond prices by cut

```{r}
ggplot(aes(x=price), data = diamonds)+
  geom_histogram(color = 'black', fill = '#F79420', binwidth = 500)+
  facet_wrap(~cut)

#Note: The code below was used to find granular information to answer the questions below:
by(diamonds$price, diamonds$cut, max)
by(diamonds$price, diamonds$cut, min)
by(diamonds$price, diamonds$cut, median)
```
  
### a) Which cut has the highest priced diamond?
Premium

### b) Which cut has the lowest priced diamond?
Premium and ideal

### c) Which cut has the lowest median price?
Ideal

## Problem F - Change the Facet wrap parameter so that the y-axis in histograms is not fixed
```{r}
ggplot(aes(x=price), data = diamonds)+
  geom_histogram(color = 'black', fill = '#F79420', binwidth = 500)+
  facet_wrap(~cut, scales = "free_y")
```

## Problem G - Create a histogram of price per carat and facet it by cut. Adjust the bin width and transform the scale of x-axis using log10
```{r}
ggplot(aes(x=price/carat), data = diamonds)+
  geom_histogram(color = 'black', fill = '#F79420')+
  facet_wrap(~cut, scales = "free_y")+
  xlab('Price per carat')+
  scale_x_log10()
```

## Problem H - Investigate the price of diamonds
```{r}
# Investigate the price of diamonds using box plots,
# numerical summaries, and one of the following categorical
# variables: cut, clarity, or color.

# There won't be a solution video for this
# exercise so go to the discussion thread for either
# BOXPLOTS BY CLARITY, BOXPLOT BY COLOR, or BOXPLOTS BY CUT
# to share you thoughts and to
# see what other people found.

# You can save images by using the ggsave() command.
# ggsave() will save the last plot created.
# For example...
#                  qplot(x = price, data = diamonds)
#                  ggsave('priceHistogram.png')

# ggsave currently recognises the extensions eps/ps, tex (pictex),
# pdf, jpeg, tiff, png, bmp, svg and wmf (windows only).

# Copy and paste all of the code that you used for
# your investigation, and submit it when you are ready.
# =================================================================

#I will do it by color to make it different
qplot(x=color, y=price, data = diamonds, geom = "boxplot")

by(diamonds$price, diamonds$color, quantile)
by(diamonds$price, diamonds$color, IQR)
```
### a) What is the price range for the middle 50% of diamonds with color D?
First quartile (25%) - 911.0
Third quartile (75%) - 4213.5

### b) What is the price range for the middle 50% of diamonds with color J?
First quartile (25%) - 1860.5
Third quartile (75%) - 7695.0

### c) What is the IQR for diamonds with the best color?
3302.5

### d) What is the IQR for diamonds with the worst color?
5834.5



## Problem I - Investigate the price per carat of diamonds across the different colors of diamonds using boxplots
```{r}
ggplot(aes(x=color, y=price/carat), data=diamonds)+
  geom_boxplot()
```

## Problem J Investigate the weight of diamonds (carat) using a frequency polygon. Use different **bin widths** to see how the frequency polygon changes. What carat size has a count greater than 2000? Check all that apply?

```{r}
ggplot(aes(x=carat), data=diamonds)+
  geom_freqpoly(binwidth = .1)+
  scale_x_continuous(lim=c(0,2.5), breaks=seq(0,5,0.1))+
  scale_y_continuous(breaks=seq(0,10500,1000))
```

### What carat size has a count greater than 2,000?
**0.3 and 1.01**. To find the answer,  I changed the bin width and the limits to check the graph under different resolutions

# Gapminder Dataset
The task requires downloading a data set of my choice and creating 2-5 plots that makes use of techniques from this lesson.

I decided to take the **Working Hours per Week** dataset from Gapminder, save it as a CSV file, tidy it up and then analyse it.

### Step 1 - Load the CSV into a dataframe, then data_table
```{r}
working_hours <- read.csv("indicator_hours per week.csv")
working_hours <- tbl_df(working_hours)

# Note: Dplyr and Tidyr are already loaded
```

### Step 2 - Tidy the data
```{r}
# The code below renames the country column correctly, deletes the empty row and the empty column at the end of the CSV
working_hours <- working_hours %>% 
  rename(Country = Working.hours.per.week) %>% 
  select(-X)

working_hours <- working_hours[working_hours$Country != "",]

# The code below creates a new tableframe to tidy the data
working_hours <- gather(working_hours, Year, Hours, -Country)
working_hours <- tbl_df(working_hours)
working_hours <- working_hours %>% mutate(Year = substr(Year,2,5))
```

### Step 3 - Draw some insights about the data
```{r}
n_distinct(working_hours$Country)
summary(working_hours$Hours)
sum(!is.na(working_hours$Hours))
```
We have ```r sum(!is.na(working_hours$Hours))``` observations across ```r n_distinct(working_hours$Country)``` countries between ```r min(working_hours$Year)``` and ```r max(working_hours$Year)```. 

The minimum number of hours was ```r min(working_hours$Hours)``` and the maximum was ```r max(working_hours$Hours)``` during the period, with a median of ```r median(working_hours$Hours)``` hours and a mean of ```r mean(working_hours$Hours)```. 


### Step 3 - Visualize the data in a histogram
```{r}
ggplot(data = working_hours, aes(x=Hours))+
  geom_histogram(color = 'black', fill = '#F79420', binwidth = 1)+
  scale_x_continuous(limits = c(25,60), breaks = seq(25,60,5))
```


The distribution has a fat tail towards longer hours. It would be useful to split the distribution of hours work by year and country to see the evolution over time globally and also by country

### Step 4 - Visualize the data by country and by year
```{r}
ggplot(data = working_hours, aes(x=Hours))+
  geom_histogram(color = 'black', fill = '#F79420', binwidth = 1)+
  scale_x_continuous(limits = c(25,60), breaks = seq(25,60,5))+
  facet_wrap(~Country, scales = "free")
```

Some countries have paucity of data which prevents drawing significant insight on changes in trends. However, for those with decent data counts (e.g. France, Germany, Korea...), it is interesting to see that some countries have seen some fluctuations between 1980 and 2007 - e.g. Japan, the Korean Republic has seen significant shifts in average weekly working hours, whereas other countries, such as New Zealand or Italy haven't seen as much of a change.

```{r}
ggplot(data = working_hours, aes(x=Hours))+
  geom_histogram(color = 'black', fill = '#F79420', binwidth = 1)+
  scale_x_continuous(limits = c(25,60), breaks = seq(25,60,5))+
  facet_wrap(~Year, scales = "free")
```

An interesting observation is that the country with the longest average working hours, Korea, has seen its average working week decline substantially, and that since 1987, the median working week has been between 32 and 35 hours per week in the countries sampled 

### Step 5 - View time series for selected countries to see movement over time

```{r}
ggplot(data=subset(working_hours, Country %in% c('Japan', 'Korea, Rep.', 'France', 'United Kingdom', 'Italy')), aes(x=Year, y=Hours, color=Country, group=Country))+
  geom_line()+
  scale_y_continuous(breaks=seq(20,60,1))
```

The countries sampled above have seen obvious declines in average working hours except for Italy and the UK, which have not seen as significant fluctutations as other countries sampled between 1980 and 2007.

### Step 6 - View data as boxplot
```{r}
ggplot(data=subset(working_hours, Country %in% c('Japan', 'Korea, Rep.', 'France', 'United Kingdom', 'Italy')), aes(x=Country, y=Hours, color=Country, group=Country))+
  geom_boxplot()+
  ylab('Average Hours Worked per Week - 1980-2007')

selected_countries <- working_hours %>% 
  filter(Country %in% c('Japan', 'Korea, Rep.', 'France', 'United Kingdom', 'Italy'))

by(selected_countries$Hours,selected_countries$Country,summary)
```
The boxplot comes to the same conclusion as the time series - Korea has seen the widest fluctuations and by far the highest median hours worked, while Italy and the UK have relatively limited variance over time. 

# Distribution of Facebook friends' birthdays

Note: I used the birthdaysExample.csv file

### Step 1 - Load the CSV file, arrange the dates in order and tidy the data, separate the day, month and year
```{r}
birthdays <- read.csv('birthdaysExample.csv')
birthdays <- arrange(birthdays, dates)
birthdays <- separate(birthdays, dates, c("month","day", "year"), sep="/", remove = FALSE)
```

### Step 2 - Create a frequency table of the birthdays and separate the day, month and year
```{r}
birthday_freq <- count(birthdays, dates)
birthday_freq <- separate(birthday_freq, dates, c("month","day", "year"), sep="/", remove = FALSE)
```

### Step 3 - Answer the questions

#### How many people share my birthday?
My birthday is 28 August. Using the below, I can see that there are two people who share my birthday

```{r}
ggplot(data = subset(birthdays, month == 8), aes(x=day))+
  geom_bar()
```

#### Which month contains the most number of birthdays?
```{r}
ggplot(birthdays, aes(x=month))+
  geom_bar()
```
March has the most number of birthdays according to the barplot

#### Which day of the year has the most number of birthdays?
```{r}
ggplot(birthdays, aes(x=dates))+
         geom_bar()+
  coord_flip()
```
Zooming in by looking at month by month, I can see that the 6th February, 22nd May and 16 July are the days with the most number of birthdays

#### Do you have at least 365 friends that have birthdays on everyday of the year?
No, as there are not even 365 observations in my frequency table

